{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c0474f6-2447-4a28-bd47-8e242b5d7a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Potential Interview Questions\n",
    "\n",
    "- What is repartitioning in spark ? \n",
    "- What is coalesce in spark ?\n",
    "- Which one will you choose and why ?\n",
    "- Repartitioning vs coalesce ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e7cc82-5b83-4a48-9f46-433ef6162730",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###### What is repartitioning in Spark?\n",
    "\n",
    "```Repartitioning is the process of redistributing data across a cluster to achieve a more optimal data partitioning.```\n",
    "\n",
    "* In Spark, the repartition method redistributes data across a specified number of partitions. It can be used to either<br>\n",
    "  increase or decrease the number of partitions.\n",
    "* The repartition() function is used to increase or decrease the number of partitions in a DataFrame.\n",
    "*  This method performs a full shuffle of data across all the nodes. It creates partitions of more or less equal in size.<br> \n",
    "   This is a costly operation given that it involves data movement all over the network.\n",
    "\n",
    "###### What is coalesce in Spark?\n",
    "```In Spark, coalesce is a method that reduces the number of partitions in a DataFrame.```\n",
    "\n",
    "* It does this by shuffling the data using a Hash Partitioner and adjusting it into existing partitions.<br>\n",
    "  This means that it can only decrease the number of partitions.\n",
    "* It avoids a full shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f821296c-c351-4a99-8a9b-253b950a8197",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Here are some screenshots from the instructor of this lecture video\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1zD6XUnVGIluVZMpUoHbzbl0omFnBGg4c\" alt=\"drawing\" style=\"width:500px;\"/><br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1dGLprdrvvtZX2qrd8CFtd2rLRQn4PYNi\" alt=\"drawing\" style=\"width:500px;\"/><br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1cNwp5-gcLDnWtJQP2eyJXXQt4EnjzRXP\" alt=\"drawing\" style=\"width:500px;\"/><br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1hOuZNQAESD9ZSolSGwGbaKFKFg2LiPlS\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "##### Some Additional resources to know more about these two concepts\n",
    "\n",
    "[Medium Article 1](https://medium.com/@tomhcorbin/repartitioning-in-spark-repartition-vs-coalesce-5e2fde5fa471#:~:text=The%20repartition%20method%20in%20Spark,partitions%20(which%20is%20200))\n",
    "\n",
    "[Medium Article 2](https://medium.com/@deepa.account/spark-coalesce-and-repartition-c61cc027ff64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac4ba70-787c-4a89-b522-46f161c0a004",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Additional Note:\n",
    "\n",
    "[What is default Parallelism parameter?](https://subhamkharwal.medium.com/pyspark-the-factor-of-cores-e884b2d5af6c#:~:text=Default%20Parallelism%20is%20a%20very%20standout%20factor%20in%20Spark%20executions,cores%20we%20have%20for%20execution.)\n",
    "\n",
    "In Spark, default parallelism is the total number of cores available for execution. The default value for this configuration<br>\n",
    "is the number of cores on all nodes in a cluster. On local, it is set to the number of cores on your system.\n",
    "\n",
    "[Go to this link to know something more on default Parallelism](https://kontext.tech/article/1149/differences-between-sparksqlshufflepartitions-and-sparkdefaultparallelism)\n",
    "\n",
    "[What is maxPartitionBytes Parameter](https://kontext.tech/article/1150/about-spark-configuration-sparksqlfilesmaxpatitionbytes#:~:text=Spark%20configuration%20property%20spark.,%2C%20ORC%2C%20CSV%2C%20etc.)\n",
    "\n",
    "* In Apache Spark, the maxPartitionBytes parameter is a configurable setting that determines the maximum size of each partition in bytes.<br>\n",
    "It specifies the upper limit to control the partition size and avoid excessively large or small partitions.\n",
    "* The default value is set to 128 MB (134217728 bytes).\n",
    "\n",
    "##### Some common resources for all the above topics\n",
    "\n",
    "[Linked In article](https://www.linkedin.com/pulse/understanding-partitioning-spark-3-levels-taral-desai/)\n",
    "\n",
    "[Medium Article](https://oindrila-chakraborty88.medium.com/introduction-to-partition-in-apache-spark-66e005c6e15d)\n",
    "\n",
    "[ProjectPro](https://www.projectpro.io/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297#:~:text=In%20Spark%2C%20one%20should%20carefully,with%20the%20number%20of%20partitions.)\n",
    "\n",
    "##### Some bonus links from Google bard on personal doubts\n",
    "\n",
    "[Question 1](https://g.co/bard/share/5e56c8770f46)\n",
    "\n",
    "[Question 2-4](https://g.co/bard/share/8e399bbc4b4f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a32170c-888e-46a3-867d-c6f935aa3016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the flight data \n",
    "\n",
    "flight_df = spark.read.format(\"csv\")\\\n",
    "               .option(\"header\",\"true\")\\\n",
    "               .option(\"inferSchema\",\"true\")\\\n",
    "               .option(\"mode\",\"FAILFAST\")\\\n",
    "               .load(\"/FileStore/tables/flight_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c314b61b-0266-49fd-a41d-dfa63a1bed2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|count|\n",
      "+------------------------+-------------------+-----+\n",
      "|United States           |Romania            |15   |\n",
      "|United States           |Croatia            |1    |\n",
      "|United States           |Ireland            |344  |\n",
      "|Egypt                   |United States      |15   |\n",
      "|United States           |India              |62   |\n",
      "|United States           |Singapore          |1    |\n",
      "|United States           |Grenada            |62   |\n",
      "|Costa Rica              |United States      |588  |\n",
      "|Senegal                 |United States      |40   |\n",
      "|Moldova                 |United States      |1    |\n",
      "|United States           |Sint Maarten       |325  |\n",
      "|United States           |Marshall Islands   |39   |\n",
      "|Guyana                  |United States      |64   |\n",
      "|Malta                   |United States      |1    |\n",
      "|Anguilla                |United States      |41   |\n",
      "|Bolivia                 |United States      |30   |\n",
      "|United States           |Paraguay           |6    |\n",
      "|Algeria                 |United States      |4    |\n",
      "|Turks and Caicos Islands|United States      |230  |\n",
      "|United States           |Gibraltar          |1    |\n",
      "+------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the flight_df dataframe\n",
    "flight_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b5a73bf-caae-4cf2-898c-446b788e3d62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 256"
     ]
    }
   ],
   "source": [
    "# Count the number of record in dataframe\n",
    "flight_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a79c304-f928-4cf0-976c-56e0ba03d9c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 1"
     ]
    }
   ],
   "source": [
    "# To know the number of partitions we have to use getNumPartitions function which returns the number of partitions in RDD.\n",
    "# Since we can't apply getNumPartitions directly on dataframe so at first we need to convert it to RDD which shown below.\n",
    "\n",
    "flight_df.rdd.getNumPartitions()\n",
    "\n",
    "# To know exactly why it gave 1 partition refer to above link named \"Question 2-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7f5870-ed80-4d5e-85a5-0ab5e2eaabbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartitioning the flight_df with 4 partitions\n",
    "partitioned_flight_df = flight_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22d4161-c72c-44c0-81da-83b4b1032530",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   64|\n",
      "|          1|   64|\n",
      "|          2|   64|\n",
      "|          3|   64|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the number of records with it's corresponding partitionId\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "partitioned_flight_df.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314da5ea-dcb0-4eb1-a77c-9bd877276ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using repartition() method you can also do the partition by single column name, or multiple columns.\n",
    "# Syntax : df.repartition(num_partitions, column)\n",
    "partitioned_on_column = flight_df.repartition(300,\"ORIGIN_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21225853-59f9-411a-acb1-936ca73df4e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: <bound method RDD.getNumPartitions of MapPartitionsRDD[68] at javaToPython at NativeMethodAccessorImpl.java:0>"
     ]
    }
   ],
   "source": [
    "# Checking the total partition of newly created dataframe after repartition by column\n",
    "partitioned_on_column.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a23575-12c8-4556-9e50-4a2a80986ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|    1|\n",
      "|          2|    2|\n",
      "|          7|    1|\n",
      "|         10|    1|\n",
      "|         13|    1|\n",
      "|         15|    2|\n",
      "|         16|    2|\n",
      "|         19|    2|\n",
      "|         22|    1|\n",
      "|         28|    1|\n",
      "|         31|    1|\n",
      "|         39|    1|\n",
      "|         43|    1|\n",
      "|         44|    1|\n",
      "|         45|    2|\n",
      "|         48|    1|\n",
      "|         53|    1|\n",
      "|         55|    1|\n",
      "|         65|    1|\n",
      "|         70|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the total number of records per partitions.\n",
    "# When the total number of records falls below the specified partition count in Spark, the framework compensates\n",
    "# by placing null values in the additional partitions.\n",
    "\n",
    "partitioned_on_column.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5263c4a5-e2e5-44c4-9e40-5aabaaab601a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new dataframe with 8 partitions to demonstrate the coalesce function\n",
    "coalesce_flight_df = flight_df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214a514c-7b8b-43fb-8cc5-8bff2146aaa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   32|\n",
      "|          1|   32|\n",
      "|          2|   32|\n",
      "|          3|   32|\n",
      "|          4|   32|\n",
      "|          5|   32|\n",
      "|          6|   32|\n",
      "|          7|   32|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The partition will be evenely distributed as we have used the repartition\n",
    "coalesce_flight_df.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7b426f-4792-493f-aaa0-e0301bae59d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# So we have 8 a dataframe with 8 partitions now let's redistribute the data using coalesce\n",
    "# Let's say we need 3 partitions as we can only descrease the number of partition in coalesce\n",
    "# Note : coalesce only takes one argument i.e number of partition we want\n",
    "\n",
    "three_coalesce_flight_df = coalesce_flight_df.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07005ae3-641f-4d26-bb59-a89046dc26d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   64|\n",
      "|          1|   96|\n",
      "|          2|   96|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As we know coalesce will unevenenly distribute the data\n",
    "three_coalesce_flight_df.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b32fae-bd96-4631-9b75-b853734d5495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   85|\n",
      "|          1|   86|\n",
      "|          2|   85|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When utilizing repartitioning with the same partition count (e.g., 3), Spark ensures an even distribution of records\n",
    "# across the partitions, in contrast to the specific behavior observed when using coalesce. \n",
    "\n",
    "# Consider the following example to witness the uniform distribution achieved through repartitioning.\n",
    "three_repartitioned_flight_df = coalesce_flight_df.repartition(3)\n",
    "\n",
    "# Display the total number of records corresponding to it's partitionId\n",
    "three_repartitioned_flight_df.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf46cbf-1d56-4f03-b79d-123fb1e0a322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: 8"
     ]
    }
   ],
   "source": [
    "# Let's see what happens when we try to increase the number of partition using coalesce.\n",
    "# I have used the coalesce_flight_df which has 8 partitions.\n",
    "coalesce_flight_df.coalesce(10).rdd.getNumPartitions()\n",
    "# As you can see we can't increase the total number of partition in coalesce."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12.Repartition vs Coalesce",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
