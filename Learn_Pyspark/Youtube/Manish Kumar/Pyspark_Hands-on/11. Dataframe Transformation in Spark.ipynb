{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c34395cb-3af8-4fad-a79a-e8aaffb0fb29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Some screenshots from this lecture videos\n",
    "![today's topic](https://drive.google.com/uc?id=1a0WF6uKcBRZ8_FFIOTkUdmVpoppqfvlY)\n",
    "![Schema](https://drive.google.com/uc?id=1B3Rh-PASPrFFv65yZSlU9hqiM9MVx3Zb)\n",
    "![Row_object](https://drive.google.com/uc?id=18B_1slKjgzpLfl3tKKp7kxolmPFKyEWU)\n",
    "![Row_creation](https://drive.google.com/uc?id=13eh_amnLTGDcMTXuDXxH9A8mwfR81LAc)\n",
    "![columns](https://drive.google.com/uc?id=1lmu2dONNVFr4nK35kLv8SjzROyUgz8e_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6af111-c008-4322-bed9-9c19a7081510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Soumya| 23| 15000|      Odisha|nominee1|\n",
      "|  2| Jyotsna| 23| 19000|      Mumbai|nominee2|\n",
      "|  3|Pratisha| 17| 20000|     Kolkata|   India|\n",
      "|  4|  Pritam| 22|100000|Uttarpradesh|   India|\n",
      "|  5|  Vikash| 31| 30000|        null|nominee5|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df  = spark.read.format(\"csv\")\\\n",
    "                           .option(\"header\",\"true\")\\\n",
    "                           .option(\"inferSchema\",\"true\")\\\n",
    "                           .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                           .load('/FileStore/tables/employee_details.csv')\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b486efbb-f783-47df-9305-8c1ec6ced2a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- nominee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c24914e1-1ba0-45d9-9f93-a39069cb0024",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Print column list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3b4e3f-ca40-4d52-a6f4-754342355683",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: ['id', 'name', 'age', 'salary', 'address', 'nominee']"
     ]
    }
   ],
   "source": [
    "employee_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b45508-92eb-4fc7-9c8b-bcc9723776bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Row Object\n",
    "A Row object in PySpark represents a record in a DataFrame or an element in an RDD of tuples.\n",
    "It is an ordered collection of fields that can be accessed starting at index 0.\n",
    "\n",
    "We can create a Row object using the Row() method. \n",
    "For example, the following code creates a Row object with three fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f93719-7e4e-437c-b224-e500c7bf766c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(name=\"Alice\", age=25, city=\"London\")\n",
    "\n",
    "# We can access the fields of a Row object using the attribute syntax.\n",
    "# For example, the following code prints the name of the person in the Row object:\n",
    "print(row.name)\n",
    "# We can also access the fields of a Row object using the index syntax. \n",
    "# For example, the following code prints the age of the person in the Row object:\n",
    "print(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9e6d9e-d093-407c-b2fe-39be63b16f86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Row objects can be used in a variety of ways in PySpark.<br>\n",
    "For example, you can use them to create new DataFrames, to filter existing DataFrames,and to perform aggregations on DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a478cc-4300-4583-b928-2d353b5ac1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+-------+-------+\n",
      "| id|  Name|Age|Salary|Address|Nominee|\n",
      "+---+------+---+------+-------+-------+\n",
      "|  1|Ramesh| 24| 50000|  India|Santosh|\n",
      "|  2|Suresh| 30|500000| Canada|Sailesh|\n",
      "+---+------+---+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframe using Row object\n",
    "from pyspark.sql import Row\n",
    "rows = [Row(id = 1,Name = \"Ramesh\",Age = 24,Salary = 50000,Address = \"India\",Nominee = \"Santosh\"),Row(id = 2,Name = \"Suresh\",Age = 30,Salary = 500000,Address = \"Canada\",Nominee = \"Sailesh\")]\n",
    "df = spark.createDataFrame(rows)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a51bff1f-1c2c-48b3-ac40-219ebca05932",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Different ways of selecting columns in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa955c2-fe97-479b-94f9-e5b4c87eb17b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  Using string form in select method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ed9659-9a53-44c7-bf20-7a8e0a121f46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|  Soumya|\n",
      "| Jyotsna|\n",
      "|Pratisha|\n",
      "|  Pritam|\n",
      "|  Vikash|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using String form\n",
    "employee_df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b29930e1-72c6-4589-887a-c0da5d6a2fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  Using col method within select method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e9f203-60e8-4664-8aa9-61673f804a70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|  Soumya|\n",
      "| Jyotsna|\n",
      "|Pratisha|\n",
      "|  Pritam|\n",
      "|  Vikash|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "employee_df.select(col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6358df-2396-4924-9bb6-3da451ad0515",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "\u001b[0;32m<command-2659229039539279>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[0;31m# Issue in selecting column using string form:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[0;31m# Suppose my requirement is to add 5 to the id column in the dataframe then and if we use string form in the below way it will lead an analysis exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 4\u001b[0;31m \u001b[0memployee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id + 5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n",
       "\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n",
       "\u001b[1;32m   2107\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   2108\u001b[0m         \"\"\"\n",
       "\u001b[0;32m-> 2109\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m   2110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n",
       "\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id + 5` cannot be resolved. Did you mean one of the following? [`id`, `address`, `age`, `name`, `nominee`];\n",
       "'Project ['id + 5]\n",
       "+- Relation [id#19,name#20,age#21,salary#22,address#23,nominee#24] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-2659229039539279>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Issue in selecting column using string form:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Suppose my requirement is to add 5 to the id column in the dataframe then and if we use string form in the below way it will lead an analysis exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0memployee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id + 5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \"\"\"\n\u001b[0;32m-> 2109\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id + 5` cannot be resolved. Did you mean one of the following? [`id`, `address`, `age`, `name`, `nominee`];\n'Project ['id + 5]\n+- Relation [id#19,name#20,age#21,salary#22,address#23,nominee#24] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id + 5` cannot be resolved. Did you mean one of the following? [`id`, `address`, `age`, `name`, `nominee`];\n'Project ['id + 5]\n+- Relation [id#19,name#20,age#21,salary#22,address#23,nominee#24] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reason behind using col method \n",
    "# Issue in selecting column using string form:\n",
    "# Suppose my requirement is to add 5 to the id column in the dataframe then and if we use string form in the below way it will lead an AnalysisException.\n",
    "employee_df.select(\"id + 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6ff4ad-bee0-470d-81ca-2ce160c5f48f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(id + 5)|\n",
      "+--------+\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col method resolves the above issue\n",
    "employee_df.select(col(\"id\")+5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b5acc2c-43a6-4d59-a234-791e9de2f0b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(id + 5)|\n",
      "+--------+\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Another twist is here that we can also resolve this issue in another way i.e using expr function\n",
    "# Basically it takes in a string argument and executes a SQL-like expression and returns a pyspark Column data type. \n",
    "# That means in earlier scenario where we simply write employee_df.select(\"id + 5\").show() which was giving AnalysisException because pyspark was not getting the column\n",
    "# named \"id + 5\"(as we know select function only take column(s) name as argument) but here as written above expr function takes string argument\n",
    "# and upon execute it as SQL-like expression and returns column which is required argument for select statement\n",
    "employee_df.select(expr(\"id + 5\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ad594e-510a-4800-a444-8ef81e6cd8b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  Using the [] operator\n",
    "\n",
    "Note: Picking columns from a DataFrame this way is handy when joining tables, especially<br> \n",
    "if their column names don't match. It helps us easily refer to the right table for each column <br> during the join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddb0641-4b57-4fc0-b891-00bf5af2521d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "| 15000|\n",
      "| 19000|\n",
      "| 20000|\n",
      "|100000|\n",
      "| 30000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(employee_df[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58167834-787d-4d0a-a979-353e2767e110",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  Using .(dot)\n",
    "This is also useful in same scenario i.e during the join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629a63fe-b912-46f1-9e63-6039282d0769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|     address|\n",
      "+------------+\n",
      "|      Odisha|\n",
      "|      Mumbai|\n",
      "|     Kolkata|\n",
      "|Uttarpradesh|\n",
      "|        null|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(employee_df.address).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d928dc9-5d92-4044-b8f8-3cd2e167730e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Selecting Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99adf806-b34a-4af5-94cd-04574f4ac490",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|  Soumya| 15000|\n",
      "|  2| Jyotsna| 19000|\n",
      "|  3|Pratisha| 20000|\n",
      "|  4|  Pritam|100000|\n",
      "|  5|  Vikash| 30000|\n",
      "+---+--------+------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Soumya|\n",
      "|  2| Jyotsna|\n",
      "|  3|Pratisha|\n",
      "|  4|  Pritam|\n",
      "|  5|  Vikash|\n",
      "+---+--------+\n",
      "\n",
      "+---+------------+\n",
      "| id|     address|\n",
      "+---+------------+\n",
      "|  1|      Odisha|\n",
      "|  2|      Mumbai|\n",
      "|  3|     Kolkata|\n",
      "|  4|Uttarpradesh|\n",
      "|  5|        null|\n",
      "+---+------------+\n",
      "\n",
      "+--------+--------+\n",
      "|    name| nominee|\n",
      "+--------+--------+\n",
      "|  Soumya|nominee1|\n",
      "| Jyotsna|nominee2|\n",
      "|Pratisha|   India|\n",
      "|  Pritam|   India|\n",
      "|  Vikash|nominee5|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(\"id\",\"name\",\"salary\").show()\n",
    "employee_df.select(col(\"id\"),col(\"name\")).show()\n",
    "employee_df.select(employee_df[\"id\"],employee_df[\"address\"]).show()\n",
    "employee_df.select(employee_df.name,employee_df.nominee).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eafecf3f-801c-430f-97e6-b709e50d46e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "|id |name    |age|salary|\n",
      "+---+--------+---+------+\n",
      "|1  |Soumya  |23 |15000 |\n",
      "|2  |Jyotsna |23 |19000 |\n",
      "|3  |Pratisha|17 |20000 |\n",
      "|4  |Pritam  |22 |100000|\n",
      "|5  |Vikash  |31 |30000 |\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All ways of selecting columns in a single statement\n",
    "# Note: We use all ways interchangeably based on our convenient\n",
    "employee_df.select(\"id\",col(\"name\"),employee_df[\"age\"],employee_df.salary).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88759bd8-e736-4df2-9263-94f921ff532a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Soumya| 23| 15000|      Odisha|nominee1|\n",
      "|  2| Jyotsna| 23| 19000|      Mumbai|nominee2|\n",
      "|  3|Pratisha| 17| 20000|     Kolkata|   India|\n",
      "|  4|  Pritam| 22|100000|Uttarpradesh|   India|\n",
      "|  5|  Vikash| 31| 30000|        null|nominee5|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting all the columns in dataframe\n",
    "employee_df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a554531c-d6ce-4119-aec2-448eb5d86c26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Doing aliasing and concatenation using expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61b681d-c172-44c0-98e5-9d8975b1858b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+\n",
      "|employee_id|employee_name|concat(name, nominee)|\n",
      "+-----------+-------------+---------------------+\n",
      "|          1|       Soumya|       Soumyanominee1|\n",
      "|          2|      Jyotsna|      Jyotsnanominee2|\n",
      "|          3|     Pratisha|        PratishaIndia|\n",
      "|          4|       Pritam|          PritamIndia|\n",
      "|          5|       Vikash|       Vikashnominee5|\n",
      "+-----------+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(expr(\"id as employee_id\"),expr(\"name as employee_name\"),expr(\"concat(name,nominee)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "456a2cde-ad13-4387-a95e-cf18b3776c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d0c8be-5614-475a-8625-c20b0b751f1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In order to use the dataframe in SparkSQL,at first we need to convert it to table/view\n",
    "employee_df.createOrReplaceTempView(\"employee_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07188fb-cf39-4394-b6a3-f3d65e659668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Soumya| 23| 15000|      Odisha|nominee1|\n",
      "|  2| Jyotsna| 23| 19000|      Mumbai|nominee2|\n",
      "|  3|Pratisha| 17| 20000|     Kolkata|   India|\n",
      "|  4|  Pritam| 22|100000|Uttarpradesh|   India|\n",
      "|  5|  Vikash| 31| 30000|        null|nominee5|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT * FROM employee_tbl\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361840bc-d214-420b-b929-aaeab502948e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+--------+\n",
      "| id|    name|salary| nominee|\n",
      "+---+--------+------+--------+\n",
      "|  1|  Soumya| 15000|nominee1|\n",
      "|  2| Jyotsna| 19000|nominee2|\n",
      "|  3|Pratisha| 20000|   India|\n",
      "|  4|  Pritam|100000|   India|\n",
      "|  5|  Vikash| 30000|nominee5|\n",
      "+---+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT id,name,salary,nominee FROM employee_tbl\n",
    "\"\"\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11. Dataframe Transformation in Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
