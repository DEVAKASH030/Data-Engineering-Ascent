{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d397c9bd-f935-4165-a265-764d11869bd0",
     "showTitle": true,
     "title": "Potential interview Questions"
    }
   },
   "source": [
    "\n",
    "1. What is json data and how to read it in spark?\n",
    "2. What if I have 3 keys in all line and 4 keys in one line?\n",
    "3. What is multiline and line-delimited JSON?\n",
    "4. Which one works faster - multiline or line-delimited?\n",
    "5. How to convert nested JSON into spark dataframe?\n",
    "6. What will happen if I have corrupted JSON file or invited JSON file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa514ab-ea3a-4dec-bbeb-1731129ad9a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What is JSON data\n",
    "------------------\n",
    "\n",
    "JSON, short for JavaScript Object Notation, is a lightweight data interchange format. It uses a human-readable text format to represent data objects consisting of \n",
    "key-value pairs. \n",
    "\n",
    "***Find the below figure where Mahish has given a brief introduction on JSON data***\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1f-9OKUvPAwIZgI86tqy9fleFaBRu1jPD\" width=\"700\" height=\"500\">\n",
    "\n",
    "[CLICK HERE](https://chat.openai.com/share/59558685-6f99-46b8-80ec-de51a6be3f71) to know more about benefits of JSON over CSV(Thanks ChatGPTðŸ’—)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd68c710-679c-4b42-8400-cd4cbad372bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Read JSON file in PySpark\n",
    "\n",
    "line_delimited_df = spark.read.format(\"json\")\\\n",
    "                              .option(\"inferSchema\",\"true\")\\\n",
    "                              .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                              .load('/FileStore/tables/line_delimited_json.json')\n",
    "line_delimited_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4ef5df-e050-48f5-a344-0376c0378e42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+------+\n|age|gender|    name|salary|\n+---+------+--------+------+\n| 20|  null|  Manish| 20000|\n| 25|  null|  Nikita| 21000|\n| 16|  null|  Pritam| 22000|\n| 35|  null|Prantosh| 25000|\n| 67|     M|  Vikash| 40000|\n+---+------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# What if I have 3 keys in all line and 4 keys in one line?\n",
    "line_delimited_extrafield_df = spark.read.format(\"json\")\\\n",
    "                              .option(\"inferSchema\",\"true\")\\\n",
    "                              .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                              .load('/FileStore/tables/line_delimited_json_extrafield.json')\n",
    "line_delimited_extrafield_df.show()\n",
    "\n",
    "#Takeways: If there is one extra field for any record then it will set null for rest of the record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f53a77-96c2-4d91-87e9-63955a30272e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What is Multiline and line-delimited JSON?\n",
    "-------------------------------------------\n",
    "In PySpark (the Python API for Apache Spark), multiline JSON and line-delimited JSON are two common formats used for reading \n",
    "and writing JSON data.\n",
    "\n",
    "***Multiline JSON:*** \n",
    "\n",
    "Multiline JSON is a format where each JSON object occupies multiple lines in a file. This format is useful \n",
    "when dealing with large JSON objects or when each line of the file represents a distinct JSON document.\n",
    "\n",
    " ***line-delimited JSON:***\n",
    "\n",
    " Line-delimited JSON, also known as JSON Lines or newline-delimited JSON, is a format where each line of a file contains a \n",
    " standalone JSON object. It's a compact and efficient way to store multiple JSON documents in a single file./\n",
    "\n",
    " [Source](https://chat.openai.com/share/fe6d4e3e-5379-44d5-987c-7ed29dd3de37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4613e71-d6c4-404f-b8e7-993db15b701b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Reading Multiline JSON\n",
    "\n",
    "# Note:\n",
    "# PySpark JSON data source API provides the multiline option to read records from multiple lines. By default, \n",
    "# PySpark considers every record in a JSON file as a fully qualified record in a single line.\n",
    "# If our JSON data is spread across multiple lines, we can use the multiline option to read it correctly. To do this,\n",
    "# we have to set the multiline option to true when reading the JSON file.\n",
    "\n",
    "multiline_json_df = spark.read.format(\"json\")\\\n",
    "                              .option(\"inferSchema\",\"true\")\\\n",
    "                              .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                              .option(\"multiline\",\"true\")\\\n",
    "                              .load('/FileStore/tables/Multi_line_correct.json')\n",
    "# It will throw an exception if multiline option is not mentioned while reading the multiline json data as by default\n",
    "# \"multipleline\" option sets to false\n",
    "multiline_json_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b34b2e-4eba-4b11-931f-903cf95a3a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Which works faster between multiline JSON and line-delimited JSON? and Why?\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UJsdHNweUYatVm2yTF4ke4GQAi3W2mPy\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0ffdec-599c-4f28-b041-9c487701849c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n|age|  name|salary|\n+---+------+------+\n| 20|Manish| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Reading incorrect multiline json file\n",
    "multiline_incorrect_json_df = spark.read.format(\"json\")\\\n",
    "                              .option(\"inferSchema\",\"true\")\\\n",
    "                              .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                              .option(\"multiline\",\"true\")\\\n",
    "                              .load('/FileStore/tables/Multi_line_incorrect.json')\n",
    "\n",
    "# It will show only first record because the provided JSON data is not in an array format. PySpark's json reader \n",
    "# expects an array of JSON objects when reading a multiline JSON file. In our case, each JSON object is on a \n",
    "# separate line, but they are not enclosed within square brackets ([]) to form an array. So it will only read first record\n",
    "\n",
    "multiline_incorrect_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd40da6-95db-4a98-a72d-adfc27208fcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----+--------+------+\n|_corrupt_record                         |age |name    |salary|\n+----------------------------------------+----+--------+------+\n|null                                    |20  |Manish  |20000 |\n|null                                    |25  |Nikita  |21000 |\n|null                                    |16  |Pritam  |22000 |\n|null                                    |35  |Prantosh|25000 |\n|{\"name\":\"Vikash\",\"age\":67,\"salary\":40000|null|null    |null  |\n+----------------------------------------+----+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Reading corrupted JSON file\n",
    "corrupted_json_df = spark.read.format(\"json\")\\\n",
    "                              .option(\"inferSchema\",\"true\")\\\n",
    "                              .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                              .load('/FileStore/tables/corrupted_json.json')\n",
    "# It will create _corrupt_record column and will add the corrupted record.\n",
    "# The null value will be set for respective valid record in _corrupt_recod column\n",
    "corrupted_json_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06.How to read json file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
