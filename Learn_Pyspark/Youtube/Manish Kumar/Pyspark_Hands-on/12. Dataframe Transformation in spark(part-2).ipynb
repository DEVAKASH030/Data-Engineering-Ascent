{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d1654be-f0d2-49ca-bbf5-b3292f74ea51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1MTwHayokmQgl6cdICl4OHHYSWUQnQzOO\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bfd2c1-6389-4992-a90e-fac4be5db59f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n+---+--------+---+------+------------+--------+\n|id |name    |age|salary|address     |nominee |\n+---+--------+---+------+------------+--------+\n|1  |Soumya  |23 |15000 |Odisha      |nominee1|\n|2  |Jyotsna |23 |19000 |Mumbai      |nominee2|\n|3  |Pratisha|17 |20000 |Kolkata     |India   |\n|4  |Pritam  |22 |100000|Uttarpradesh|India   |\n|5  |Vikash  |31 |30000 |null        |nominee5|\n+---+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "####### Imports ######\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "####### Reading Data #######\n",
    "employee_df  = spark.read.format(\"csv\")\\\n",
    "                           .option(\"header\",\"true\")\\\n",
    "                           .option(\"inferSchema\",\"true\")\\\n",
    "                           .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                           .load('/FileStore/tables/employee_details.csv')\n",
    "####### Printing Schema #######\n",
    "employee_df.printSchema()\n",
    "\n",
    "####### Displaying the dataframe #######\n",
    "employee_df.show(truncate=False)\n",
    "\n",
    "####### Creating Temporary View #######\n",
    "employee_df.createOrReplaceTempView(\"employee_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095355c8-d073-49f2-8098-0fc95c5d0625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+\n|employee_id|    name|age|\n+-----------+--------+---+\n|          1|  Soumya| 23|\n|          2| Jyotsna| 23|\n|          3|Pratisha| 17|\n|          4|  Pritam| 22|\n|          5|  Vikash| 31|\n+-----------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Column aliasing/renaming using col and alias method\n",
    "employee_df.select(col(\"id\").alias(\"employee_id\"),\"name\",\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a473d-d4af-4d65-a15e-2ab2649c78aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+\n| id|  name|age|salary|     address| nominee|\n+---+------+---+------+------------+--------+\n|  4|Pritam| 22|100000|Uttarpradesh|   India|\n|  5|Vikash| 31| 30000|        null|nominee5|\n+---+------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Finding employee whose salary is more than 20,000\n",
    "# Both methods are equivalent and serve the same purpose, with no inherent differences between them.\n",
    "# Whether you prefer a SQL-like syntax or are more comfortable with Scala, the choice between \n",
    "# filter and where largely depends on personal preference. \n",
    "# If you are accustomed to SQL,using where might align better with your familiarity. On the other hand,\n",
    "# if you are more inclined towards Scala, utilizing filter might feel more natural.\n",
    "employee_df.filter(col(\"salary\")>20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1e27f1-eb94-403f-baeb-e2a4b2aacbd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+\n| id|  name|age|salary|     address| nominee|\n+---+------+---+------+------------+--------+\n|  4|Pritam| 22|100000|Uttarpradesh|   India|\n|  5|Vikash| 31| 30000|        null|nominee5|\n+---+------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# We can also do the same using where\n",
    "employee_df.where(col(\"salary\")>20000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ea0e514-47e8-4bf1-97a9-cb21804e1cd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Applying various conditions and merging them through logical operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a42007c-c192-4135-b0c9-8f4038eb72d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+-------+\n| id|  name|age|salary|     address|nominee|\n+---+------+---+------+------------+-------+\n|  4|Pritam| 22|100000|Uttarpradesh|  India|\n+---+------+---+------+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find the employees who earn more than 20000 but age is less than 30\n",
    "employee_df.where((col(\"salary\")>20000) & (col(\"age\") < 30)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0778527-c570-4041-b9e2-077518726c06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### literals\n",
    "A literal (also known as a constant) represents a fixed data value.<br> \n",
    "lit() function is used to add constant or literal value as a new column to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf20a580-ab2b-48ab-86a4-a420eb68c601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+\n| id|    name|age|salary|     address| nominee|Religion|\n+---+--------+---+------+------------+--------+--------+\n|  1|  Soumya| 23| 15000|      Odisha|nominee1|   Hindu|\n|  2| Jyotsna| 23| 19000|      Mumbai|nominee2|   Hindu|\n|  3|Pratisha| 17| 20000|     Kolkata|   India|   Hindu|\n|  4|  Pritam| 22|100000|Uttarpradesh|   India|   Hindu|\n|  5|  Vikash| 31| 30000|        null|nominee5|   Hindu|\n+---+--------+---+------+------------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(\"*\",lit(\"Hindu\").alias(\"Religion\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4563ec40-f8b5-4622-bb79-5007fed20caf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### withColumn()\n",
    "\n",
    "The withColumn() function in PySpark is used to add, replace, or update columns(include changing the data type)<br>\n",
    "in a DataFrame. It returns a new DataFrame with the updated column.\n",
    "\n",
    "The syntax for the withColumn() function is as follows:<br>\n",
    "**df = df.withColumn(new_column_name/existing_column, expression)**\n",
    "\n",
    "The new_column_name/existing_column is the name of the new column/existing column respectively, and the<br>\n",
    "expression is the expression that will be used to populate the new column. The expression can be a constant<br>\n",
    "value, a PySpark column, or a PySpark expression.\n",
    "\n",
    "*Remember : It takes only two argument as you can see in syntax.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7fefa18-4918-4485-aba6-8e934e69c29f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+---------+\n| id|    name|age|salary|     address| nominee|last_name|\n+---+--------+---+------+------------+--------+---------+\n|  1|  Soumya| 23| 15000|      Odisha|nominee1|    Singh|\n|  2| Jyotsna| 23| 19000|      Mumbai|nominee2|    Singh|\n|  3|Pratisha| 17| 20000|     Kolkata|   India|    Singh|\n|  4|  Pritam| 22|100000|Uttarpradesh|   India|    Singh|\n|  5|  Vikash| 31| 30000|        null|nominee5|    Singh|\n+---+--------+---+------+------------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#Adding new column using withColumn()\n",
    "employee_df.withColumn(\"last_name\",lit(\"Singh\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21df6c05-ddc9-4a52-8960-092f6e5509c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### withColumnRenamed()\n",
    "The withColumnRenamed() method in PySpark is used to rename an existing column in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e652d6b-1ab4-4a32-88d5-1937d332a3d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+------+------------+--------+\n|employee_id|    name|age|salary|     address| nominee|\n+-----------+--------+---+------+------------+--------+\n|          1|  Soumya| 23| 15000|      Odisha|nominee1|\n|          2| Jyotsna| 23| 19000|      Mumbai|nominee2|\n|          3|Pratisha| 17| 20000|     Kolkata|   India|\n|          4|  Pritam| 22|100000|Uttarpradesh|   India|\n|          5|  Vikash| 31| 30000|        null|nominee5|\n+-----------+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Changing column name using withColumnRenamed\n",
    "new_employee_df = employee_df.withColumnRenamed(\"id\",\"employee_id\")\n",
    "\n",
    "# Displaying the new employee dataframe with changed name\n",
    "new_employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14df2f8-9776-41ad-9f87-6f4f4d0301a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4ecea8-7fe6-4bcf-bda7-b8df6f5e9643",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: double (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Casting column data type.\n",
    "# As we know withColumn method is used to update the column so now we can use\n",
    "# withColumn to change the data type of the column\n",
    "employee_df.withColumn(\"id\",col(\"salary\").cast(\"string\"))\\\n",
    "           .withColumn(\"salary\",col(\"salary\").cast(\"double\"))\\\n",
    "           .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcc725a-d4b6-4b9e-97a6-1cd9286d4db8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Let's do the same thing using expr\n",
    "employee_df_expr = employee_df.withColumn(\"id\", expr(\"CAST(id AS long)\")) \\\n",
    "                         .withColumn(\"age\", expr(\"CAST(age AS string)\"))\n",
    "\n",
    "employee_df_expr.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0c0887-7477-400b-9251-3061d8e20cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Removing the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba52a99e-0e0b-4c84-8fca-ab346a8f6846",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n| id|    name|age|salary|\n+---+--------+---+------+\n|  1|  Soumya| 23| 15000|\n|  2| Jyotsna| 23| 19000|\n|  3|Pratisha| 17| 20000|\n|  4|  Pritam| 22|100000|\n|  5|  Vikash| 31| 30000|\n+---+--------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# We can use drop method to remove the columns from the dataframe\n",
    "employee_df.drop(\"address\",col(\"nominee\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "024c505f-f527-4a06-b599-485c351d7084",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SparkSQL\n",
    "Let's do all above transformations in SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45135ff-e11e-43f0-b000-a05890a86a6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+-------+\n| id|  name|age|salary|     address|nominee|\n+---+------+---+------+------------+-------+\n|  4|Pritam| 22|100000|Uttarpradesh|  India|\n+---+------+---+------+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Displaying the employee details whose salary is more than 20000 and age is less than 30\n",
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM employee_tbl\n",
    "WHERE salary > 20000 and age < 30\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1665d0d-7f54-49c4-9b88-23b8784efd6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+---------+\n| id|    name|age|salary|     address| nominee|last_name|\n+---+--------+---+------+------------+--------+---------+\n|  1|  Soumya| 23| 15000|      Odisha|nominee1|    kumar|\n|  2| Jyotsna| 23| 19000|      Mumbai|nominee2|    kumar|\n|  3|Pratisha| 17| 20000|     Kolkata|   India|    kumar|\n|  4|  Pritam| 22|100000|Uttarpradesh|   India|    kumar|\n|  5|  Vikash| 31| 30000|        null|nominee5|    kumar|\n+---+--------+---+------+------------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's add literals value in by creating new column\n",
    "spark.sql(\"\"\"\n",
    "SELECT *,\"kumar\" as last_name \n",
    "FROM employee_tbl\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9880ad7-2059-4376-9937-9cc74cb3f699",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+----------+\n| id|    name|age|salary|     address| nominee|updated_id|\n+---+--------+---+------+------------+--------+----------+\n|  1|  Soumya| 23| 15000|      Odisha|nominee1|         6|\n|  2| Jyotsna| 23| 19000|      Mumbai|nominee2|         7|\n|  3|Pratisha| 17| 20000|     Kolkata|   India|         8|\n|  4|  Pritam| 22|100000|Uttarpradesh|   India|         9|\n|  5|  Vikash| 31| 30000|        null|nominee5|        10|\n+---+--------+---+------+------------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Adding the new columns\n",
    "spark.sql(\"\"\"\n",
    "SELECT *,id + 5 as updated_id \n",
    "FROM employee_tbl\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dca5604-83c1-4543-963d-681e2f2c4764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+------+------------+--------+\n|employee_id|    name|age|salary|     address| nominee|\n+-----------+--------+---+------+------------+--------+\n|          1|  Soumya| 23| 15000|      Odisha|nominee1|\n|          2| Jyotsna| 23| 19000|      Mumbai|nominee2|\n|          3|Pratisha| 17| 20000|     Kolkata|   India|\n|          4|  Pritam| 22|100000|Uttarpradesh|   India|\n|          5|  Vikash| 31| 30000|        null|nominee5|\n+-----------+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Renaming the column\n",
    "spark.sql(\"\"\"\n",
    "SELECT id as employee_id,name,age,salary,address,nominee \n",
    "FROM employee_tbl\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06c0560-07a3-44c2-b71f-dd29947a35d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: double (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Cast the data type of the column\n",
    "spark.sql(\"\"\"\n",
    "SELECT id,name,age,cast(salary as double),address,nominee\n",
    "FROM employee_tbl\n",
    "\"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816edad7-fa89-4416-a3b1-7ac834a39687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+\n| id|    name|age|salary|     address|\n+---+--------+---+------+------------+\n|  1|  Soumya| 23| 15000|      Odisha|\n|  2| Jyotsna| 23| 19000|      Mumbai|\n|  3|Pratisha| 17| 20000|     Kolkata|\n|  4|  Pritam| 22|100000|Uttarpradesh|\n|  5|  Vikash| 31| 30000|        null|\n+---+--------+---+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# For removing the column, just make it absence in select statement and it will be not present in the resulted dataframe\n",
    "updated_emp_df = spark.sql(\"\"\"\n",
    "SELECT id,name,age,salary,address \n",
    "FROM employee_tbl\n",
    "\"\"\")\n",
    "\n",
    "# As we can see nominee column is not present in this new dataframe\n",
    "updated_emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99c9e573-e777-4507-9586-cac63e9b30b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12. Dataframe Transformation in spark(part-2)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
