{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33849f5c-a341-4960-a590-2334c9bf5227",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Potential Interview Questions**\n",
    "\n",
    "1. Have you worked with corrupted record?\n",
    "2. When do you say that it's corrupted record?\n",
    "3. What happens when we encounter with corrupted records in different read mode?\n",
    "4. How can we print bad records?\n",
    "5. Where do you store corrupted records and how can we access it later?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a62950f-99aa-4053-94aa-58045087859b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " When do you say that it's corrupted record?\n",
    " ---------------------------------------------\n",
    " \n",
    " **Example of Corrupted record in JSON**\n",
    "\n",
    " {\n",
    "  key:value\n",
    " }\n",
    "\n",
    " {\n",
    "  key:value\n",
    " }\n",
    "\n",
    " {\n",
    "  key:value\n",
    "\n",
    "\n",
    "**Example of Corrupted record in CSV**\n",
    "\n",
    "id,name,age,salary,address,nominee\n",
    "\n",
    "1,Soumya,23,15000,Odisha,nominee1\n",
    "\n",
    "2,Jyotsna,23,19000,Mumbai,nominee2\n",
    "\n",
    "3,Pratisha,17,20000,Kolkata,India,nominee3\n",
    "\n",
    "4,Pritam,22,100000,Uttarpradesh,India,nominee4\n",
    "\n",
    "5,Vikash,31,30000,,nominee5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b68b533-9b1a-45fc-ade2-673ea4556bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What happens when we encounter with corrupted records in different read mode?\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c527f5e-e0c4-423c-bd2f-96c639479124",
     "showTitle": true,
     "title": "Total how many records will there in PERMISSIVE mode?"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in permissive mode is  5\n"
     ]
    }
   ],
   "source": [
    "employee_details  = spark.read.format(\"csv\")\\\n",
    "                           .option(\"header\",\"true\")\\\n",
    "                           .option(\"inferSchema\",\"true\")\\\n",
    "                           .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                           .load('/FileStore/tables/employee_details.csv')\n",
    "\n",
    "print('Total records in permissive mode is ',employee_details .count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d3499b-04c9-4518-bad9-58c6013aa80a",
     "showTitle": true,
     "title": "Total how many records will there in DROPMALFORMED mode?"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in permissive mode is  3\n"
     ]
    }
   ],
   "source": [
    "employee_details = spark.read.format(\"csv\")\\\n",
    "                           .option(\"header\",\"true\")\\\n",
    "                           .option(\"inferSchema\",\"true\")\\\n",
    "                           .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "                           .load('/FileStore/tables/employee_details.csv')\n",
    "\n",
    "# employee_details .count() only checks for existence of rows, not their validity. If some rows contain malformed data,\n",
    "#  employee_details .show() might skip them while employee_details .count() includes them.\n",
    "# In order to fix this, we can use dropna() which will drop all the problematic records.\n",
    "# I have used how='all' because in our result dataframe one valid row contains null value so if we will not mention how \n",
    "# parameter as 'all' then it will drop that record as well.Refer dropna() method for detailed reason\n",
    "print('Total records in DROPMALFORMED mode is ',employee_details .dropna(how='all').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fee886-5fb5-4a5e-86a3-72931e5ac9d3",
     "showTitle": true,
     "title": "Total how many records will there in FAILFAST mode?"
    }
   },
   "outputs": [],
   "source": [
    "employee_details  = spark.read.format(\"csv\")\\\n",
    "                           .option(\"header\",\"true\")\\\n",
    "                           .option(\"inferSchema\",\"true\")\\\n",
    "                           .option(\"mode\",\"FAILFAST\")\\\n",
    "                           .load('/FileStore/tables/employee_details.csv')\n",
    "# It throws an exception when it meets corrupted records. Since there are two corrupted records in our dataframe so \n",
    "# it will throw an exception\n",
    "#employee_details .show()(Note: Commented it because it will give long error which is not suitable for viewing \n",
    "# notebook in github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ace4fc4f-10ed-462a-820a-eb9f531cb568",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "How can we print bad records?\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af2be77-91f3-4a6e-99bb-6d0ef7467078",
     "showTitle": true,
     "title": "Print bad records in DataFrame"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-------+--------+--------------+\n",
      "|id |name   |age|salary|address|nominee |corrupt_record|\n",
      "+---+-------+---+------+-------+--------+--------------+\n",
      "|1  |Soumya |23 |15000 |Odisha |nominee1|null          |\n",
      "|2  |Jyotsna|23 |19000 |Mumbai |nominee2|null          |\n",
      "|5  |Vikash |31 |30000 |null   |nominee5|null          |\n",
      "+---+-------+---+------+-------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "emp_schema = StructType([\n",
    "                      StructField(\"id\", IntegerType(), True),\n",
    "                      StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"age\", IntegerType(), True),\n",
    "                      StructField(\"salary\", IntegerType(), True),\n",
    "                      StructField(\"address\", StringType(), True),\n",
    "                      StructField(\"nominee\", StringType(), True),\n",
    "                      StructField(\"corrupt_record\",StringType(),True)\n",
    "                    ])\n",
    "\n",
    "# Note: The 'True' argument for nullable allows for null values in the columns\n",
    "\n",
    "# Creating employee details dataframe\n",
    "# It is mandatory to set the mode to PERMISSIVE in order to include the bad records in the DataFrame.\n",
    "\n",
    "employee_details  = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"corrupt_record\")\\\n",
    "    .schema(emp_schema) \\\n",
    "    .load('/FileStore/tables/employee_details.csv')\n",
    "\n",
    "employee_details.show(truncate=False)\n",
    "\n",
    "# Note: If we choose to name the column for corrupted records as \"_corrupted_record\" during schema definition,\n",
    "# we can omit this option -> \".option(\"columnNameOfCorruptRecord\", \"corrupt_record\")\". The process will still \n",
    "# function correctly.\n",
    "# We can also rename(_corrupt_record) column with any name only when using \".option(\"columnNameOfCorruptRecord\", \n",
    "# \"any_name\")\\\" otherwise it is mandatory to use \"_corrupt_record\" as column name to print the bad records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bcaad6d-b741-43ea-b0c2-0ed5c43f7205",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where do you store corrupted records and how can we access it later?\n",
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d130538-9775-4d69-950f-acf0689e9647",
     "showTitle": true,
     "title": "Store bad records"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+-------+--------+\n",
      "|id |name   |age|salary|address|nominee |\n",
      "+---+-------+---+------+-------+--------+\n",
      "|1  |Soumya |23 |15000 |Odisha |nominee1|\n",
      "|2  |Jyotsna|23 |19000 |Mumbai |nominee2|\n",
      "|5  |Vikash |31 |30000 |null   |nominee5|\n",
      "+---+-------+---+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "emp_schema = StructType([\n",
    "                      StructField(\"id\", IntegerType(), True),\n",
    "                      StructField(\"name\", StringType(), True),\n",
    "                      StructField(\"age\", IntegerType(), True),\n",
    "                      StructField(\"salary\", IntegerType(), True),\n",
    "                      StructField(\"address\", StringType(), True),\n",
    "                      StructField(\"nominee\", StringType(), True)\n",
    "                    ])\n",
    "\n",
    "# Creating employee details dataframe\n",
    "# mode is not allowed if we use \"badRecordsPath\"\n",
    "\n",
    "employee_details  = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"badRecordsPath\",\"/FileStore/tables/bad_records\")\\\n",
    "    .schema(emp_schema) \\\n",
    "    .load('/FileStore/tables/employee_details.csv')\n",
    "\n",
    "employee_details.show(truncate=False)\n",
    "\n",
    "# Note: It will always create a JSON file to store the corrupted records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae14884-cc7c-4ce7-bc15-fb5062e6e874",
     "showTitle": true,
     "title": "Check the bad records file"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/bad_records/20231212T140533/bad_records/part-00000-e9467212-69c6-415b-ac0c-63192ec4b770</td><td>part-00000-e9467212-69c6-415b-ac0c-63192ec4b770</td><td>504</td><td>1702389935000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/bad_records/20231212T140533/bad_records/part-00000-e9467212-69c6-415b-ac0c-63192ec4b770",
         "part-00000-e9467212-69c6-415b-ac0c-63192ec4b770",
         504,
         1702389935000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('/FileStore/tables/bad_records/20231212T140533/bad_records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90461e24-0178-40c5-9420-a0190c25c016",
     "showTitle": true,
     "title": "Display bad records"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
      "|path                                       |reason                                                                                                                             |record                                        |\n",
      "+-------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
      "|dbfs:/FileStore/tables/employee_details.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pratisha,17,20000,Kolkata,India,nominee3    |3,Pratisha,17,20000,Kolkata,India,nominee3    |\n",
      "|dbfs:/FileStore/tables/employee_details.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 4,Pritam,22,100000,Uttarpradesh,India,nominee4|4,Pritam,22,100000,Uttarpradesh,India,nominee4|\n",
      "+-------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_bad_records_df = spark.read.format(\"json\").load('dbfs:/FileStore/tables/bad_records/20231212T140533/bad_records/part-00000-e9467212-69c6-415b-ac0c-63192ec4b770')\n",
    "emp_bad_records_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328fd6ff-88c6-466e-b86d-36f198d82517",
     "showTitle": true,
     "title": "Only showing bad records"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|record                                        |\n",
      "+----------------------------------------------+\n",
      "|3,Pratisha,17,20000,Kolkata,India,nominee3    |\n",
      "|4,Pritam,22,100000,Uttarpradesh,India,nominee4|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "bad_records = emp_bad_records_df.select(col('record'))\n",
    "bad_records.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Handling Corrupted Records",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
