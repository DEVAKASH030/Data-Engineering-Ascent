{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "221a1e85-c919-494e-ae4f-c78750f3f0d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Group By Master class\n",
    "----------------------\n",
    "1. How group by works\n",
    "2. How to implement in Pyspark\n",
    "\n",
    "[Refer this page for more](https://sparkbyexamples.com/pyspark/pyspark-groupby-agg-aggregate-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41ebeb3-8a75-4dd4-ab50-9baa15566746",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "data = [(1,'manish',50000,'IT'),\n",
    "(2,'vikash',60000,'sales'),\n",
    "(3,'raushan',70000,'marketing'),\n",
    "(4,'mukesh',80000,'IT'),\n",
    "(5,'pritam',90000,'sales'),\n",
    "(6,'nikita',45000,'marketing'),\n",
    "(7,'ragini',55000,'marketing'),\n",
    "(8,'rakesh',100000,'IT'),\n",
    "(9,'aditya',65000,'IT'),\n",
    "(10,'rahul',50000,'marketing')]\n",
    "\n",
    "schema = ['id','name','salary','dept']\n",
    "\n",
    "employee_df = spark.createDataFrame(data = data,schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "451b7f15-5a12-47d3-b7f5-3454e4ce6db0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+\n| id|   name|salary|     dept|\n+---+-------+------+---------+\n|  1| manish| 50000|       IT|\n|  2| vikash| 60000|    sales|\n|  3|raushan| 70000|marketing|\n|  4| mukesh| 80000|       IT|\n|  5| pritam| 90000|    sales|\n|  6| nikita| 45000|marketing|\n|  7| ragini| 55000|marketing|\n|  8| rakesh|100000|       IT|\n|  9| aditya| 65000|       IT|\n| 10|  rahul| 50000|marketing|\n+---+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16af6c12-3d93-4d63-9977-17eef6c86872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|     dept|total_salary|\n+---------+------------+\n|       IT|      295000|\n|    sales|      150000|\n|marketing|      220000|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"total_salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1d6f51-88fa-4f44-a680-18bdba4b5bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I learned one interesting thing i.e if we will not import sum from pyspark.sql.functions and will try<br>\n",
    "to use sum directly as we had done in aggregate notebook then we will get error.\n",
    "\n",
    "[Click Here to know more about this](https://stackoverflow.com/questions/36719039/sum-operation-on-pyspark-dataframe-giving-typeerror-when-type-is-fine/36719760#36719760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e54599f-be18-438a-9ada-8e1d1a858ead",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+---------+------------+\n| id|   name|salary|     dept|     dept|total_salary|\n+---+-------+------+---------+---------+------------+\n|  1| manish| 50000|       IT|       IT|      295000|\n|  2| vikash| 60000|    sales|    sales|      150000|\n|  3|raushan| 70000|marketing|marketing|      220000|\n|  5| pritam| 90000|    sales|    sales|      150000|\n|  4| mukesh| 80000|       IT|       IT|      295000|\n|  6| nikita| 45000|marketing|marketing|      220000|\n|  7| ragini| 55000|marketing|marketing|      220000|\n|  8| rakesh|100000|       IT|       IT|      295000|\n| 10|  rahul| 50000|marketing|marketing|      220000|\n|  9| aditya| 65000|       IT|       IT|      295000|\n+---+-------+------+---------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# We can't get all the columns except groupBy and aggregate column\n",
    "# while using groupBy.\n",
    "# To resolve this issue we have two options either we can use window functions\n",
    "# Otherwise we have to join the original dataframe with new dataframe in which \n",
    "# groupBy is applied.\n",
    "\n",
    "new_df = employee_df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"total_salary\"))\n",
    "result_df = employee_df.join(new_df, employee_df.dept == new_df.dept)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da18b37-eb6d-485e-8f6e-27c85a31ede8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question asked in video\n",
    "question_data = [\n",
    "    (1, 'manish', 50000, 'IT', 'india'),\n",
    "    (2, 'vikash', 60000, 'sales', 'us'),\n",
    "    (3, 'raushan', 70000, 'marketing', 'india'),\n",
    "    (4, 'mukesh', 80000, 'IT', 'us'),\n",
    "    (5, 'pritam', 90000, 'sales', 'india'),\n",
    "    (6, 'nikita', 45000, 'marketing', 'us'),\n",
    "    (7, 'ragini', 55000, 'marketing', 'india'),\n",
    "    (8, 'rakesh', 100000, 'IT', 'us'),\n",
    "    (9, 'aditya', 65000, 'IT', 'india'),\n",
    "    (10, 'rahul', 50000, 'marketing', 'us')\n",
    "]\n",
    "\n",
    "question_data_schema = ['id','name','salary','dept','country']\n",
    "\n",
    "emp_df = spark.createDataFrame(question_data).toDF(*question_data_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369b2cee-72dc-474d-86a2-5f02ad0863d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+\n|country|     dept|Total_Salary|\n+-------+---------+------------+\n|  india|       IT|      115000|\n|  india|marketing|      125000|\n|  india|    sales|       90000|\n|     us|    sales|       60000|\n|     us|       IT|      180000|\n|     us|marketing|       95000|\n+-------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Solution of the asked question\n",
    "emp_df.groupBy(\"country\",\"dept\")\\\n",
    "      .agg(sum(\"salary\")\n",
    "      .alias(\"Total_Salary\"))\\\n",
    "      .sort(col(\"country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abde882-8e54-468b-a4e5-145ed99d6633",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+\n|country|dept     |Total_Salary|\n+-------+---------+------------+\n|india  |IT       |115000      |\n|us     |sales    |60000       |\n|india  |marketing|125000      |\n|india  |sales    |90000       |\n|us     |IT       |180000      |\n|us     |marketing|95000       |\n+-------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's solve it using SparkSQL\n",
    "emp_df.createOrReplaceTempView(\"emp_tbl\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          SELECT country,dept,sum(salary) as Total_Salary\n",
    "          FROM emp_tbl\n",
    "          GROUP BY country,dept\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17. groupBy in spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
