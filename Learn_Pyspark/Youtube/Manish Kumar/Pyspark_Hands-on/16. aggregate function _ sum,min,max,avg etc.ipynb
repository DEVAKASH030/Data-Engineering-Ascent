{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "047acac9-7ebf-474f-8f4b-c9aed792d941",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Todays' topic\n",
    "---------------\n",
    "Aggregation : Collecting something together and it works on numerical value\n",
    "\n",
    "I. count\n",
    "\n",
    "II. min\n",
    "\n",
    "III. max\n",
    "\n",
    "IV. sum\n",
    "\n",
    "V. avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4786edef-b02f-4fdb-ad37-a06830b5b4cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "count\n",
    "-------\n",
    "```In Spark, the count function is an action that returns the number of rows in a DataFrame.```\n",
    "\n",
    "As per the instructor count acts as both action and transfromation;\n",
    "\n",
    "For example:<br>\n",
    "while we apply count on df directly like **df.count()** then it acts as an action <br>\n",
    "while we apply it inside select like df.select(count(\"*\")) then it acts as transformation\n",
    "\n",
    "[Please refer to this bard chat](https://g.co/bard/share/a18899a59c56)\n",
    "And [This linkedin post](https://www.linkedin.com/posts/namanseth_data-transformation-count-activity-7116445887807074304-SHs-?utm_source=share&utm_medium=member_desktop)\n",
    "<br>\n",
    "Additionally if you want to get some overview on Transfromation then pleae [click here](https://www.linkedin.com/pulse/wide-vs-narrow-transformations-sparkdistributed-compute-don-hilborn/)\n",
    "\n",
    "Remember that count behaves differently in different situation as described below.<br>\n",
    "If we apply count method on single column of dataframe then it will skip **null** value in the column<br>\n",
    "Whereas if we apply count on all columns i.e df.count() then it will not skip **null** value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e6b48e-4d9b-46bc-9fb2-7471c9c69552",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "schema = ['id','name','age','sal','country','dept']\n",
    "emp_df = spark.createDataFrame(data = emp_data,schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d32f0c9-a262-4cda-9cfc-325c49ab5b7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|   sal|country|       dept|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1981bd66-0b46-450c-bdcd-11c7f215fef6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 10"
     ]
    }
   ],
   "source": [
    "# Count all th records;as you can see it didn't skip null value\n",
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640ecaac-85da-4fa1-abff-613e24fb86fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|count(name)|\n+-----------+\n|          8|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Count total number of records on name column; and as you can see it skipped null value in name column\n",
    "emp_df.select(count(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca70eaa-3aa9-4352-b47c-86f685a12c64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: DataFrame[count(country): bigint]"
     ]
    }
   ],
   "source": [
    "# As discussed above if we will use count inside select then it will act as transformation\n",
    "# And as you can see here for the below code no job is created\n",
    "emp_df.select(count(\"country\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9444f161-9e62-4162-ac15-3ff37856c631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n|total sal|maximum sal|minimum sal|\n+---------+-----------+-----------+\n|   560000|     200000|      20000|\n+---------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# We can apply other aggregate function in below way\n",
    "emp_df.select(sum(\"sal\").alias(\"total sal\"),max(\"sal\").alias(\"maximum sal\"),min(\"sal\").alias(\"minimum sal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350f896b-84c9-4f81-9c7f-36be6491e29c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+----------+\n|total sal|total number of salary|avg_salary|\n+---------+----------------------+----------+\n|560000   |8                     |70000     |\n+---------+----------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(sum(\"sal\").alias(\"total sal\"),count(\"sal\").alias(\"total number of salary\")\\\n",
    "                                            ,avg(\"sal\").cast(\"int\").alias(\"avg_salary\"))\\\n",
    "                                            .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a498781c-a528-4f8f-b994-ddc44def3e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "16. aggregate function | sum,min,max,avg etc",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
