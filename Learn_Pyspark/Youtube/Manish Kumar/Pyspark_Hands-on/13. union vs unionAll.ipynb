{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7a2048f-20f4-4fa6-887b-3774b6bdbed0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "All about union and unionAll\n",
    "-----------------------------\n",
    "1. Difference beween union and unionAll?\n",
    "2. What will happen if I change the no. of columns while unioning the data?\n",
    "3. What if column name is different?\n",
    "4. What is unionByName?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c44b3bd6-cabb-4708-adcd-868048327ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###### Dataframe union()\n",
    "union() method of the DataFrame is used to combine two DataFrame’s of the same structure/schema.<br>\n",
    " If schemas are not the same it returns an error.\n",
    "\n",
    "###### DataFrame unionAll()\n",
    "unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n",
    "\n",
    "Note: In other SQL’s, Union eliminates the duplicates but UnionAll combines two datasets including duplicate records.<br>\n",
    "But, in spark both behave the same and use [DataFrame duplicate function to remove duplicate rows.](https://stackoverflow.com/questions/29022530/why-would-i-want-union-over-unionall-in-spark-for-schemardds)\n",
    "\n",
    "###### Bonus fact from spark documentation : \n",
    "In Spark 3.0, the Dataset and DataFrame API unionAll is no longer deprecated. It is an alias for union.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa1fd9f-bf92-43a2-9f29-1440b8e3f2cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------+\n| id|  name|  sal|mgr_id|\n+---+------+-----+------+\n| 10|  Anil|50000|    18|\n| 11| Vikas|75000|    16|\n| 12| Nisha|40000|    18|\n| 13| Nidhi|60000|    17|\n| 14| Priya|80000|    18|\n| 15| Mohit|45000|    18|\n| 16|Rajesh|90000|    10|\n| 17| Raman|55000|    16|\n| 18|   Sam|65000|    17|\n+---+------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(10, 'Anil', 50000, 18),\n",
    "        (11, 'Vikas', 75000, 16),\n",
    "        (12, 'Nisha', 40000, 18),\n",
    "        (13, 'Nidhi', 60000, 17),\n",
    "        (14, 'Priya', 80000, 18),\n",
    "        (15, 'Mohit', 45000, 18),\n",
    "        (16, 'Rajesh', 90000, 10),\n",
    "        (17, 'Raman', 55000, 16),\n",
    "        (18, 'Sam', 65000,   17)]\n",
    "\n",
    "# Defining schema\n",
    "schema = ['id', 'name', 'sal', 'mgr_id']\n",
    "\n",
    "# Creating dataframe\n",
    "manager_df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# Show the dataframe\n",
    "manager_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa7c11d-8a56-46fb-b5f3-030e4c34eae6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 9"
     ]
    }
   ],
   "source": [
    "# Count the number of record in dataframe           \n",
    "manager_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a5c121-5434-4609-a305-e0be76f06083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n| id| name|  sal|mgr_id|\n+---+-----+-----+------+\n| 19|Sohan|50000|    18|\n| 20| Sima|75000|    17|\n+---+-----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17)]\n",
    "\n",
    "# Defining schema\n",
    "schema1 = ['id', 'name', 'sal', 'mgr_id']\n",
    "\n",
    "# Creating dataframe\n",
    "manager_df1 = spark.createDataFrame(data = data1,schema = schema1)\n",
    "\n",
    "# Show the dataframe\n",
    "manager_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7576ca8a-b34b-491e-8ff9-3166c6e67a33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------+\n| id|  name|  sal|mgr_id|\n+---+------+-----+------+\n| 10|  Anil|50000|    18|\n| 11| Vikas|75000|    16|\n| 12| Nisha|40000|    18|\n| 13| Nidhi|60000|    17|\n| 14| Priya|80000|    18|\n| 15| Mohit|45000|    18|\n| 16|Rajesh|90000|    10|\n| 17| Raman|55000|    16|\n| 18|   Sam|65000|    17|\n| 19| Sohan|50000|    18|\n| 20|  Sima|75000|    17|\n+---+------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Apply union transformation on both the dataframe\n",
    "manager_df.union(manager_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fb8b7e-b031-49ce-94f0-5960e77d0df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: 11"
     ]
    }
   ],
   "source": [
    "# Counting number of records after unioning both the dataframe\n",
    "manager_df.union(manager_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70217c1-8025-421b-a36c-e7c20e2e31de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data2 = [(10, 'Anil', 50000, 18),\n",
    "        (11, 'Vikas', 75000, 16),\n",
    "        (12, 'Nisha', 40000, 18),\n",
    "        (13, 'Nidhi', 60000, 17),\n",
    "        (14, 'Priya', 80000, 18),\n",
    "        (15, 'Mohit', 45000, 18),\n",
    "        (16, 'Rajesh', 90000, 10),\n",
    "        (17, 'Raman', 55000, 16),\n",
    "        (18, 'Sam', 65000,   17),\n",
    "        (18, 'Sam', 55000,   17),\n",
    "        (18, 'Sam', 65000,   17)]\n",
    "\n",
    "# Defining schema\n",
    "schema2 = ['id', 'name', 'sal', 'mgr_id']\n",
    "\n",
    "# Creating duplicate_manager_df1 dataframe by keeping some duplicate value of manager_df1\n",
    "duplicate_manager_df1 = spark.createDataFrame(data=data2, schema=schema2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c896466-85e1-4c0e-8f8b-457a97bba33e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------+\n| id|  name|  sal|mgr_id|\n+---+------+-----+------+\n| 10|  Anil|50000|    18|\n| 11| Vikas|75000|    16|\n| 12| Nisha|40000|    18|\n| 13| Nidhi|60000|    17|\n| 14| Priya|80000|    18|\n| 15| Mohit|45000|    18|\n| 16|Rajesh|90000|    10|\n| 17| Raman|55000|    16|\n| 18|   Sam|65000|    17|\n| 18|   Sam|55000|    17|\n| 18|   Sam|65000|    17|\n| 19| Sohan|50000|    18|\n| 20|  Sima|75000|    17|\n+---+------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's try to apply both union and unionAll on manager_df1 and duplicate_manager_df1\n",
    "duplicate_manager_df1.union(manager_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684676e9-21a5-43b8-93b7-473361e3c83e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: 13"
     ]
    }
   ],
   "source": [
    "# Count total record after applying union on duplicate dataframe\n",
    "duplicate_manager_df1.union(manager_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604ef7d8-47fe-41c8-92c9-803b9a9b747c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: 13"
     ]
    }
   ],
   "source": [
    "# Count total record after applying unionAll on duplicate dataframe\n",
    "duplicate_manager_df1.unionAll(manager_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b0978ea-c577-4c0c-ad46-d1b8868c7edb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# As demonstrated above, despite employing duplicate values, both the `union` and `unionAll` operations \n",
    "# yield all records from both dataframes. This observation highlights that in Spark, both operations exhibit similar \n",
    "# behavior, as previously mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37315ba5-af26-41ad-8f13-ad951931a819",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's do the same operation in sparkSQL\n",
    "# Before that we have to convert it to temporary view or table\n",
    "manager_df1.createOrReplaceTempView(\"manager_tbl\")\n",
    "duplicate_manager_df1.createOrReplaceTempView(\"duplicate_manager_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfce03de-5822-4ca1-83cb-d509543ae706",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n|total_record_after_union|\n+------------------------+\n|                      12|\n+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# As you can see in SparkSQL, the union merged two table and removed duplicate(in dataframe it was showing 13 records)\n",
    "spark.sql(\"\"\"\n",
    "         SELECT COUNT(*) AS total_record_after_union\n",
    "         FROM(\n",
    "          SELECT * FROM manager_tbl\n",
    "          UNION\n",
    "          SELECT * FROM duplicate_manager_tbl\n",
    "         )\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ce7531-d868-431c-96a2-fd09b6204c10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n|total_record_after_union_all|\n+----------------------------+\n|                          13|\n+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# As you can see in SparkSQL, the uniona ll merged two tables but didn't remove duplicate(in dataframe it was showing 13 records)\n",
    "spark.sql(\"\"\"\n",
    "         SELECT COUNT(*) AS total_record_after_union_all\n",
    "         FROM(\n",
    "          SELECT * FROM manager_tbl\n",
    "          UNION ALL\n",
    "          SELECT * FROM duplicate_manager_tbl\n",
    "         )\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd0486b-03b4-4ffa-af18-c3fb5d1e2c17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: 10"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM duplicate_manager_tbl\n",
    "          UNION\n",
    "          SELECT * FROM duplicate_manager_tbl\n",
    "          \"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f00882-8250-437f-9e18-5dba81179d90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### My understanding on above query\n",
    "The SparkSQL code produced 10 unique records by using UNION on both views. The 'duplicate_manager_tbl'<br>\n",
    "originally had 11 records, including one duplicate. The UNION operation combined the tables, resulting in 22 records,<br>\n",
    "and then removed the two duplicates, leaving a final output of 10 records.\n",
    "\n",
    "[Where did I get to know this](https://discuss.codecademy.com/t/what-happens-if-tables-we-perform-union-on-have-duplicate-rows/356107)\n",
    "###### Some additional resources to understand these topics better\n",
    "\n",
    "[StrataScratch Blog](https://www.stratascratch.com/blog/sql-union-vs-union-all-differences-you-need-to-know/#:~:text=One%20important%20feature%20of%20UNION,UNION%20to%20remove%20that%20row.)\n",
    "\n",
    "[Chat with Google Bard](https://g.co/bard/share/a44e01018f49)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1ced58-0008-45b6-97cc-501aa543c083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n| id|  sal|mgr_id| name|\n+---+-----+------+-----+\n| 19|50000|    18|Sohan|\n| 20|75000|    17| Sima|\n+---+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "schema3 = ['id','sal','mgr_id','name']\n",
    "wrong_manager_df = spark.createDataFrame(data = wrong_column_data,schema = schema3)\n",
    "wrong_manager_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2696ada-f84f-496d-909e-bb3aed5f5b52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n| id|  sal|mgr_id| name|\n+---+-----+------+-----+\n| 19|50000|    18|Sohan|\n| 20|75000|    17| Sima|\n| 19|Sohan| 50000|   18|\n| 20| Sima| 75000|   17|\n+---+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's see what happens we change the order of the column\n",
    "# Let's use union on wrong_manager_df and manager_df1\n",
    "wrong_manager_df.union(manager_df1).show()\n",
    "# As you can see Spark places the record as is, without knowledge of the positioning for the 'id' and 'name' \n",
    "# values in corresponding column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8010873f-9046-4ab3-9340-3ec7858ec1e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "unionByName\n",
    "-------------\n",
    "```To resolve the above issue spark provides another method i.e unionByName```<br>\n",
    "\n",
    "The unionByName() function in PySpark is used to union two DataFrames when you have column names in a different order or<br>\n",
    "even if you have missing columns in any DataFrame, in other words, this function resolves columns by name (not by position).\n",
    "\n",
    "Syntax : ```DataFrame.unionByName(other, allowMissingColumns=False)```\n",
    "\n",
    "The allowMissingColumns parameter is a boolean value that specifies whether or not to allow missing columns in<br>\n",
    "the other DataFrame. If this parameter is set to True, then any missing columns in the other DataFrame will be added<br>\n",
    "to the current DataFrame with a default value of null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311a5f85-23f5-432a-bdbe-3983a5455798",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n| id|  sal|mgr_id| name|\n+---+-----+------+-----+\n| 19|50000|    18|Sohan|\n| 20|75000|    17| Sima|\n| 19|50000|    18|Sohan|\n| 20|75000|    17| Sima|\n+---+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# As you can see here it resolves the above issue by placing the values in corresponding column\n",
    "wrong_manager_df.unionByName(manager_df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c29b14-e97b-4598-84ed-05c55585f1ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Limitation of unionByName\n",
    "But it has some limitation i.e it cannot be used to combine DataFrames with different schemas. If the two DataFrames<br>\n",
    "have different columns, or if the columns have different data types, the unionByName() function will fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80a9c62-81d9-469c-856f-ae58aabd211d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n| id|  sal|mgr_id|  nam|\n+---+-----+------+-----+\n| 19|50000|    18|Sohan|\n| 20|75000|    17| Sima|\n+---+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Creating new dataframe with updated schema to demonstrate the limitation of unionByName\n",
    "wrong_column_data1=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "schema4 = ['id','sal','mgr_id','nam']\n",
    "updated_schema_wrong_manager_df = spark.createDataFrame(data = wrong_column_data1,schema = schema4)\n",
    "updated_schema_wrong_manager_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a462ccbb-4595-4ed3-b952-5fddd89dfa7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2310222119595342>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# As you can see here it is throwing an exception\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mupdated_schema_wrong_manager_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanager_df1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n",
       "\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3685\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"nam\" among (id, name, sal, mgr_id)."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2310222119595342>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# As you can see here it is throwing an exception\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mupdated_schema_wrong_manager_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanager_df1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3685\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"nam\" among (id, name, sal, mgr_id).",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot resolve column name \"nam\" among (id, name, sal, mgr_id).",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As you can see here it is throwing an Analysis exception because both the dataframe have different schema\n",
    "updated_schema_wrong_manager_df.unionByName(manager_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56361a3d-39e1-4797-9dd8-ece04ab3f599",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe with extra column named bonus\n",
    "data3 = [(10, 'Anil', 50000, 18,20),\n",
    "               (11, 'Vikas', 75000, 16,32),\n",
    "               (12, 'Nisha', 40000, 18,35),\n",
    "               (13, 'Nidhi', 60000, 17,15)]\n",
    "schema6 = ['id','name','sal','mgr_id','bonus']\n",
    "manager_df2 = spark.createDataFrame(data = data3,schema = schema6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2222337-3432-4b37-9f26-baf62fa2d85d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2310222119595345>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Let's check what happens if we change the number of columns while applying union\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mmanager_df1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanager_df2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3601\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n",
       "'Union false, false\n",
       ":- LogicalRDD [id#326L, name#327, sal#328L, mgr_id#329L], false\n",
       "+- LogicalRDD [id#1273L, name#1274, sal#1275L, mgr_id#1276L, bonus#1277L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2310222119595345>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Let's check what happens if we change the number of columns while applying union\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmanager_df1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmanager_df2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3601\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n'Union false, false\n:- LogicalRDD [id#326L, name#327, sal#328L, mgr_id#329L], false\n+- LogicalRDD [id#1273L, name#1274, sal#1275L, mgr_id#1276L, bonus#1277L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n'Union false, false\n:- LogicalRDD [id#326L, name#327, sal#328L, mgr_id#329L], false\n+- LogicalRDD [id#1273L, name#1274, sal#1275L, mgr_id#1276L, bonus#1277L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check what happens if we change the number of columns while applying union\n",
    "manager_df1.union(manager_df2).show()\n",
    "# As you can clearly see here it is throwing Analysis Exception saying mismatch of no. of columns.\n",
    "# So we have to always apply union on dataframes with same no. of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3cbb37-6670-4907-bfff-dcd02471fe1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+-----+\n| id| name|  sal|mgr_id|bonus|\n+---+-----+-----+------+-----+\n| 19|Sohan|50000|    18| null|\n| 20| Sima|75000|    17| null|\n| 10| Anil|50000|    18|   20|\n| 11|Vikas|75000|    16|   32|\n| 12|Nisha|40000|    18|   35|\n| 13|Nidhi|60000|    17|   15|\n+---+-----+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# We can resolve the above issue using unionByName but we have to set True in allowMissingColumns parameter\n",
    "manager_df1.unionByName(manager_df2,allowMissingColumns=True).show()\n",
    "# Note: As per the video we can do it without using unionByName i.e by selecting only common column\n",
    "# i.e manager_df1.select('id','name','sal','mgr_id').union(manager_df2).show()\n",
    "# But don't know why it is giving the same error as above as column mismatch. I will check it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4063682-d5a6-4f20-8452-d2609bc46730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+-----+\n| id|  sal|mgr_id| name|bonus|\n+---+-----+------+-----+-----+\n| 19|50000|    18|Sohan|   10|\n| 20|75000|    17| Sima|   20|\n+---+-----+------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with extra column named bonus and changing the order of the columns\n",
    "wrong_column_data3=[(19 ,50000, 18,'Sohan',10),\n",
    "(20 ,75000,  17,'Sima',20)]\n",
    "schema6 = ['id','sal','mgr_id','name','bonus']\n",
    "\n",
    "wrong_manager_df2 = spark.createDataFrame(data = wrong_column_data3,schema = schema6)\n",
    "wrong_manager_df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74c64b88-4a6d-4f05-954c-da873ffe5566",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n| id|  sal|mgr_id| name|\n+---+-----+------+-----+\n| 19|50000|    18|Sohan|\n| 20|75000|    17| Sima|\n| 19|Sohan| 50000|   18|\n| 20| Sima| 75000|   17|\n+---+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Same thing was not working with manager_df1 and manager_df2\n",
    "# But as we can see this is working here means although wrong_manager_df2 has one more additional\n",
    "# column than manager_df1 but by selecting only common column we can combine both the dataframe\n",
    "# using union. Since the order of the columns are different in both the df for which it is not showing\n",
    "# corresponding value in respective column but we can fix this using unionByName as we have already\n",
    "# done earlier.\n",
    "# Note: I will try to figure out why same thing was not working on manager_df1 and manager_df2\n",
    "wrong_manager_df2.select('id','sal','mgr_id','name').union(manager_df1).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13. union vs unionAll",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
