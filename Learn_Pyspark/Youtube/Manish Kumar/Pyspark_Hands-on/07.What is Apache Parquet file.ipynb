{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8fe83f6-7695-4c27-b1d6-9ec32aed7e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Potential Interview Questions:\n",
    "1. What is parquet file format?\n",
    "2. Why do we need parquet?\n",
    "3. How to read parquet file?\n",
    "4. What makes parquet default choice?\n",
    "5. What encoding is done on data?\n",
    "6. What compression techniques are used?\n",
    "7. How to optimize the parquet file?\n",
    "8. What is row group, column and pages ?\n",
    "9. How projection pruning and predicate pushdown works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02bc0892-016c-44c9-968e-4da9c2989bba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**What is Parquet file format**\n",
    "- Parquet is a columnar storage file format that is optimized for use with big data processing frameworks\n",
    "- Parquet organizes data by columns rather than by rows. This columnar storage format is well-suited for analytics\n",
    "  and data processing<br> tasks where only specific columns need to be read and processed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f389c3-94be-4749-9e05-94eec013a968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**What is row-based file format and column-based file format?**\n",
    "\n",
    "Row-based and column-based file formats refer to the way data is organized and stored within a file.\n",
    "- In row-based storage, data is organized and stored in rows. Each row contains all the data for a single record or tuple.\n",
    "  Ex:CSV, TSV, JSON, and Avro \n",
    "- In column-based storage, data is organized and stored in columns. Each column contains all the values for a specific attribute\n",
    "  across all records.<br>Parquet, and ORC file are columnar file formats.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1FrpDlPGea0HL0MMK2qXgFDrcGo1xG9Mj\" alt=\"drawing\" style=\"width:700px;\"/>\n",
    "\n",
    "\n",
    "> *Note: The principle of \"write once, read many\" is a common philosophy in the context of big data. Apache Parquet and Apache \n",
    "ORC<br> (Optimized Row Columnar),are designed with principles that align with \"write once, read many\". They provide columnar storage and <br>compression, which are well-suited for analytical queries.*\n",
    "\n",
    "|OLAP|OLTP|\n",
    "|-----|-----|\n",
    "|Online Analytical Processing|Online Transactional Processing|\n",
    "|Only few columns read|Write: Insert,update,delete|\n",
    "|column-based file format|row-based file format|\n",
    "\n",
    "[Click here to check detailed explanation of the mentioned topic](https://chat.openai.com/share/cb02bd87-ca82-4595-8f76-e78110c20a69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1074d329-b8e2-4f14-b9f3-1bd2f9bf0cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------------------+-------------------+-----+\n",
      "|United States                   |Romania            |1    |\n",
      "|United States                   |Ireland            |264  |\n",
      "|United States                   |India              |69   |\n",
      "|Egypt                           |United States      |24   |\n",
      "|Equatorial Guinea               |United States      |1    |\n",
      "|United States                   |Singapore          |25   |\n",
      "|United States                   |Grenada            |54   |\n",
      "|Costa Rica                      |United States      |477  |\n",
      "|Senegal                         |United States      |29   |\n",
      "|United States                   |Marshall Islands   |44   |\n",
      "|Guyana                          |United States      |17   |\n",
      "|United States                   |Sint Maarten       |53   |\n",
      "|Malta                           |United States      |1    |\n",
      "|Bolivia                         |United States      |46   |\n",
      "|Anguilla                        |United States      |21   |\n",
      "|Turks and Caicos Islands        |United States      |136  |\n",
      "|United States                   |Afghanistan        |2    |\n",
      "|Saint Vincent and the Grenadines|United States      |1    |\n",
      "|Italy                           |United States      |390  |\n",
      "|United States                   |Russia             |156  |\n",
      "+--------------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read parquet file\n",
    "# One of the key reasons why no additional options are often required when reading Parquet files in PySpark is because \n",
    "# of its self-describing nature and rich metadata\n",
    "df = spark.read.parquet('/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet')\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0f6f13-581e-4a2c-bae7-9b53c6b8f5d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Indeed Parquet is a column-based file format which is most efficient file format for analytical workload but there some limitation<br>\n",
    "of it which has discussed in the video(Attaching the screenshot of that particular portion)**\n",
    "\n",
    "> Suppose you have a table with 100 columns, and each column has a size of 10GB. In a columnar storage format, when you need to read<br> only \n",
    "> the 1st,2nd, and last columns, the cost and time associated with scanning through the 3rd column to the 98th column become inefficient.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12aWtNhZ7xakiZaoSZj8IKlsjvvf18V1u\" alt=\"drawing\" style=\"width:700px;\"/>\n",
    "\n",
    "**So to resolve this issue Parquet uses a hybrid storage format which sequentially stores chunks of columns,lending \n",
    "to high performance<br> when selecting and filtering data.**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ANDtigOKXiuea_-6i4XmbIVXHz1s3B9D\" alt=\"drawing\" style=\"width:700px;\"/>\n",
    "\n",
    "**Note**\n",
    "- Parquet is a structured data file format.\n",
    "- It comes with binary form means normally we can't read the contents of the file using normal editor of the system.\n",
    "- And of course it's a columnar-based file format but it uses hybrid approach for better efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61598226-d5ff-4d57-994d-74ba2d041658",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*Below figure demonstrates the internal structure of the parquet file.*\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Ntsvj4Mt99CRw3ndRNFRaIqMCDIXX8HH\" alt=\"drawing\" style=\"width:550px;\"/>\n",
    "\n",
    "- The data in a Parquet file is broken into horizontal slices called RowGroups\n",
    "- Each RowGroup contains a single ColumnChunk for each column in the schema\n",
    "\n",
    "**For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two RowGroups<br> for a total of 6 ColumnChunks.**\n",
    "<img src=\"https://drive.google.com/uc?id=1eLvt23RddsoyLfnkQxuBhx3WbFtVyeGK\" alt=\"drawing\" style=\"width:350px;\"/>\n",
    "\n",
    "*Also take a look at the below picture for better visualization on organization of data in parquet file*\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1-hrSNC-BQb5etzP7YWzwfJciufx35H3V\" alt=\"drawing\" style=\"width:800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ba0af2e-e20c-4ba8-825c-619d7367c831",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Compression**\n",
    "\n",
    "File compression is the act of taking a file and making it smaller. In Parquet, compression is performed column by column<br> \n",
    "and it is built to support flexible compression options and extendable encoding schemas per data type – e.g., different encoding<br> \n",
    "can be used for compressing integer and string data.\n",
    "\n",
    "Parquet data can be compressed using these encoding methods:\n",
    "\n",
    "- Dictionary encoding: this is enabled automatically and dynamically for data with a small number of unique values.\n",
    "- Bit packing: Storage of integers is usually done with dedicated 32 or 64 bits per integer. This allows more \n",
    "  efficient storage of small integers.\n",
    "- Run length encoding (RLE): when the same value occurs multiple times, a single value is stored once along with the number \n",
    "  of occurrences.<br> Parquet implements a combined version of bit packing and RLE, in which the encoding switches based on which\n",
    "  produces the best compression results.\n",
    "\n",
    "*Below picture has shown the visualization of the encoding in Parquet file format*\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1o73zy_x_ALacL6mcP6OtTSmORx4SOky5\" alt=\"drawing\" style=\"width:750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c928b334-cb30-4caa-ad87-2f330ad95cda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Some screenshots from video:**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gupCvFH1IAVbnpvbckOVhaFflLteRQwi\" alt=\"drawing\" style=\"width:750px;\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ThWpOD317T3NO4lotetYrwVMQnvZxZ-D\" alt=\"drawing\" style=\"width:750px;\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Mai4qWJhUh63jmphe_fuPWD9bsdFmfe6\" alt=\"drawing\" style=\"width:750px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42042893-8b5c-4d97-9312-1d2e77ca888d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07.What is Apache Parquet file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
